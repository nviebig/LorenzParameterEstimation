# Test to verify we're using actual optimizers, not just gradient descent
using LorenzParameterEstimation

# Generate longer trajectory 
true_params = L63Parameters(10.0, 28.0, 8/3)
system = L63System(params=true_params, u0=[1.0, 1.0, 1.0], tspan=(0.0, 5.0), dt=0.01)
solution = integrate(system)

initial_guess = L63Parameters(8.0, 25.0, 2.5)

println("ğŸ” Testing if we're actually using different optimizers...")
println("True params: Ïƒ=10.0, Ï=28.0, Î²=2.667")
println("Initial guess: Ïƒ=8.0, Ï=25.0, Î²=2.5")
println()

# Test 1: SGD with no momentum (pure gradient descent)
println("1ï¸âƒ£  Testing SGD (no momentum)...")
results_sgd = modular_train!(
    deepcopy(initial_guess), solution,
    optimizer_config = sgd_config(learning_rate=5e-3, momentum=0.0),
    loss_function = window_rmse,
    epochs = 30,
    window_size = 100,
    verbose = false
)

# Test 2: Adam (adaptive learning rates)
println("2ï¸âƒ£  Testing Adam...")
results_adam = modular_train!(
    deepcopy(initial_guess), solution,
    optimizer_config = adam_config(learning_rate=5e-3),
    loss_function = window_rmse,
    epochs = 30,
    window_size = 100,
    verbose = false
)

# Test 3: AdamW (with weight decay)
println("3ï¸âƒ£  Testing AdamW...")
results_adamw = modular_train!(
    deepcopy(initial_guess), solution,
    optimizer_config = adamw_config(learning_rate=5e-3, weight_decay=1e-3),
    loss_function = window_rmse,
    epochs = 30,
    window_size = 100,
    verbose = false
)

println("\nğŸ“Š Results after 30 epochs:")
println("="^50)
println("Optimizer â”‚    Ïƒ     â”‚    Ï     â”‚    Î²     â”‚  Loss  ")
println("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€")
println("SGD       â”‚ $(rpad(round(results_sgd.best_params.Ïƒ, digits=3), 8)) â”‚ $(rpad(round(results_sgd.best_params.Ï, digits=3), 8)) â”‚ $(rpad(round(results_sgd.best_params.Î², digits=3), 8)) â”‚ $(round(results_sgd.metrics_history[end].train_loss, digits=3))")
println("Adam      â”‚ $(rpad(round(results_adam.best_params.Ïƒ, digits=3), 8)) â”‚ $(rpad(round(results_adam.best_params.Ï, digits=3), 8)) â”‚ $(rpad(round(results_adam.best_params.Î², digits=3), 8)) â”‚ $(round(results_adam.metrics_history[end].train_loss, digits=3))")
println("AdamW     â”‚ $(rpad(round(results_adamw.best_params.Ïƒ, digits=3), 8)) â”‚ $(rpad(round(results_adamw.best_params.Ï, digits=3), 8)) â”‚ $(rpad(round(results_adamw.best_params.Î², digits=3), 8)) â”‚ $(round(results_adamw.metrics_history[end].train_loss, digits=3))")

println("\nğŸ“ˆ Parameter changes from initial:")
println("SGD:   Î”Ïƒ=$(round(results_sgd.best_params.Ïƒ - 8.0, digits=3)), Î”Ï=$(round(results_sgd.best_params.Ï - 25.0, digits=3)), Î”Î²=$(round(results_sgd.best_params.Î² - 2.5, digits=3))")
println("Adam:  Î”Ïƒ=$(round(results_adam.best_params.Ïƒ - 8.0, digits=3)), Î”Ï=$(round(results_adam.best_params.Ï - 25.0, digits=3)), Î”Î²=$(round(results_adam.best_params.Î² - 2.5, digits=3))")
println("AdamW: Î”Ïƒ=$(round(results_adamw.best_params.Ïƒ - 8.0, digits=3)), Î”Ï=$(round(results_adamw.best_params.Ï - 25.0, digits=3)), Î”Î²=$(round(results_adamw.best_params.Î² - 2.5, digits=3))")

# Check if results are actually different
Ïƒ_diff_sgd_adam = abs(results_sgd.best_params.Ïƒ - results_adam.best_params.Ïƒ)
Ïƒ_diff_adam_adamw = abs(results_adam.best_params.Ïƒ - results_adamw.best_params.Ïƒ)

println("\nğŸ”¬ Analysis:")
if Ïƒ_diff_sgd_adam > 0.01 || Ïƒ_diff_adam_adamw > 0.01
    println("âœ… Optimizers are DIFFERENT! SGD vs Adam difference: $(round(Ïƒ_diff_sgd_adam, digits=4))")
    println("âœ… This proves we're using actual optimizers, not just gradient descent!")
else
    println("âš ï¸  Optimizers seem similar. Difference: $(round(Ïƒ_diff_sgd_adam, digits=4))")
    println("   This might indicate we're still doing vanilla gradient descent...")
end