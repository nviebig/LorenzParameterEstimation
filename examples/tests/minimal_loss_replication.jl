# Create a minimal test that exactly replicates the loss.jl logic
using LorenzParameterEstimation
using Enzyme

println("ğŸ”¬ Minimal Loss.jl Replication Test")
println("="^40)

true_params = L63Parameters(10.0, 28.0, 8/3)
test_params = L63Parameters(9.5, 27.0, 2.8)

system = L63System(params=true_params, u0=[1.0, 1.0, 1.0], tspan=(0.0, 2.0), dt=0.01)
solution = integrate(system)

# Replicate the exact logic from compute_gradients_modular
window_start = 1
window_length = 50

# Extract window data (exactly like loss.jl)
u0 = solution[window_start]
window_end = window_start + window_length
target_window = solution.u[window_start:window_end, :]
dt = solution.system.dt

# Convert to compatible types (exactly like loss.jl)
T = Float64
Ïƒ0, Ï0, Î²0 = T(test_params.Ïƒ), T(test_params.Ï), T(test_params.Î²)
u0_vec = u0 isa Vector ? u0 : Vector{T}(u0)
target_mat = target_window isa Matrix ? target_window : Matrix{T}(target_window)

println("Data setup complete...")
println("Ïƒ0=$Ïƒ0, Ï0=$Ï0, Î²0=$Î²0")

# Create exact MAE function from loss.jl  
function mae_enzyme_exact(Ïƒ::T, Ï::T, Î²::T, u0::AbstractVector{T}, 
                         target_trajectory::AbstractMatrix{T}, 
                         window_length::Int, dt::T) where {T}
    params = L63Parameters{T}(Ïƒ, Ï, Î²)
    u = similar(u0)
    u .= u0
    
    ae = zero(T) # Absolute error accumulator
    count = 0
    epsilon = T(1e-6)  # Small value for smooth approximation
    
    @inbounds for i in 1:window_length 
        u = LorenzParameterEstimation.rk4_step(u, params, dt)
        for j in 1:3
            diff = u[j] - target_trajectory[i+1, j]
            # Use smooth approximation: sqrt(x^2 + Îµ) â‰ˆ |x| but differentiable everywhere
            ae += sqrt(diff * diff + epsilon)
            count += 1
        end
    end
    
    return ae / count
end

println("Testing enzyme call exactly like loss.jl...")

try
    grads = Enzyme.autodiff(
        Enzyme.Reverse,
        Enzyme.Const(mae_enzyme_exact),
        Enzyme.Active(Ïƒ0),
        Enzyme.Active(Ï0), 
        Enzyme.Active(Î²0),
        Enzyme.Const(u0_vec),
        Enzyme.Const(target_mat),
        Enzyme.Const(window_length),
        Enzyme.Const(dt)
    )
    
    println("Raw grads: $grads")
    println("grads[1]: $(grads[1])")
    
    # Try both extraction methods
    println("\nMethod 1 (G = grads[1]; G[1], G[2], G[3]):")
    G = grads[1]
    gÏƒ1, gÏ1, gÎ²1 = G[1], G[2], G[3]
    println("  Ïƒ=$gÏƒ1, Ï=$gÏ1, Î²=$gÎ²1")
    
    println("\nMethod 2 (grads[1][1], grads[1][2], grads[1][3]):")
    gÏƒ2, gÏ2, gÎ²2 = grads[1][1], grads[1][2], grads[1][3]
    println("  Ïƒ=$gÏƒ2, Ï=$gÏ2, Î²=$gÎ²2")
    
    # They should be the same
    if gÏƒ1 == gÏƒ2 && gÏ1 == gÏ2 && gÎ²1 == gÎ²2
        println("âœ… Both methods give the same result")
    else
        println("âŒ Methods give different results!")
    end
    
    # Check what's creating the L63Parameters
    final_grads = L63Parameters{T}(gÏƒ1, gÏ1, gÎ²1)
    println("\nFinal gradient object: $final_grads")
    println("Individual values: Ïƒ=$(final_grads.Ïƒ), Ï=$(final_grads.Ï), Î²=$(final_grads.Î²)")
    
catch e
    println("ERROR: $e")
end