{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb45ebc",
   "metadata": {},
   "source": [
    "# Theta Parameter Estimation in Modified Lorenz 63\n",
    "\n",
    "This notebook demonstrates a challenging test case for parameter estimation using the `LorenzParameterEstimation` package: learning the theta parameter in a modified Lorenz 63 system.\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "We modify the standard Lorenz 63 system by introducing a theta parameter in the y-equation:\n",
    "- dx/dt = σ(y - x)\n",
    "- dy/dt = **θ**x(ρ - z) - y  ← Modified with theta parameter\n",
    "- dz/dt = xy - βz\n",
    "\n",
    "**Key challenge**: The parameter space is fractal - for some theta values the strange attractor collapses, for others it reemerges without changing the overall shape too much.\n",
    "\n",
    "**Critical theta values of interest:**\n",
    "- θ = 1.0 (default/standard)\n",
    "- θ = 3.5 (attractor changes)\n",
    "- θ = 4.0 (different behavior)  \n",
    "- θ = 4.6 (yet another regime)\n",
    "\n",
    "**Gradient challenge**: Computing gradients through this fractal parameter space is extremely challenging, demonstrating the limits of gradient-based optimization and the robustness of the package's Enzyme.jl integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b0473",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a218ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "using Plots\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Enzyme\n",
    "\n",
    "# Import our LorenzParameterEstimation package\n",
    "using Pkg\n",
    "Pkg.activate(\"../../..\")  # Activate the main project\n",
    "using LorenzParameterEstimation\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "Random.seed!(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2fdc9",
   "metadata": {},
   "source": [
    "## Base System Setup\n",
    "\n",
    "Use the package's standard parameters and extend for theta modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430466ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use package standard parameters\n",
    "base_params = classic_params()  # Standard Lorenz 63 parameters from package\n",
    "println(\"Base Lorenz 63 parameters: σ=$(base_params.σ), ρ=$(base_params.ρ), β=$(base_params.β)\")\n",
    "println(\"Lorenz 63 with theta modification: dy/dt = θx(ρ-z) - y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0eefc",
   "metadata": {},
   "source": [
    "## Extended System: Theta Parameter\n",
    "\n",
    "Implement the Lorenz 63 system with theta parameter in the y-equation, extending the package functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4713f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theta-modified Lorenz 63 system\n",
    "# This extends the package functionality for this specific test case\n",
    "function lorenz63_theta!(du, u, theta_params, t, base_params::L63Parameters)\n",
    "    x, y, z = u\n",
    "    θ = theta_params[1]  # theta parameter to be learned\n",
    "    \n",
    "    du[1] = base_params.σ * (y - x)\n",
    "    du[2] = θ * x * (base_params.ρ - z) - y  # ← Modified with theta\n",
    "    du[3] = x * y - base_params.β * z\n",
    "end\n",
    "\n",
    "# Create a wrapper that's compatible with DifferentialEquations.jl\n",
    "function make_theta_system(base_params::L63Parameters)\n",
    "    return (du, u, theta_params, t) -> lorenz63_theta!(du, u, theta_params, t, base_params)\n",
    "end\n",
    "\n",
    "# Standard system (theta = 1.0) for comparison\n",
    "function lorenz63_standard!(du, u, p, t, base_params::L63Parameters)\n",
    "    x, y, z = u\n",
    "    du[1] = base_params.σ * (y - x)\n",
    "    du[2] = x * (base_params.ρ - z) - y\n",
    "    du[3] = x * y - base_params.β * z\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed8263",
   "metadata": {},
   "source": [
    "## Explore Fractal Parameter Space\n",
    "\n",
    "First, let's explore the fractal behavior by generating attractors for different theta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theta values of particular interest\n",
    "theta_values = [1.0, 3.5, 4.0, 4.6]\n",
    "theta_names = [\"Standard (θ=1.0)\", \"θ=3.5\", \"θ=4.0\", \"θ=4.6\"]\n",
    "\n",
    "# Initial conditions (standard attractor region)\n",
    "u0 = [1.0, 1.0, 1.0]\n",
    "tspan = (0.0, 50.0)  # Longer integration to see attractor structure\n",
    "dt = 0.01\n",
    "\n",
    "# Generate attractors for each theta value\n",
    "attractors = []\n",
    "theta_system = make_theta_system(base_params)\n",
    "\n",
    "for (i, θ) in enumerate(theta_values)\n",
    "    println(\"Generating attractor for $(theta_names[i])...\")\n",
    "    \n",
    "    prob = ODEProblem(theta_system, u0, tspan, [θ])\n",
    "    sol = solve(prob, Tsit5(), saveat=dt)\n",
    "    \n",
    "    if sol.retcode == :Success\n",
    "        push!(attractors, sol)\n",
    "        println(\"  Success: $(length(sol.u)) points\")\n",
    "    else\n",
    "        println(\"  Failed: $(sol.retcode)\")\n",
    "        push!(attractors, nothing)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e9b1d",
   "metadata": {},
   "source": [
    "## Visualize Different Theta Regimes\n",
    "\n",
    "Create plots to visualize how the attractor changes with different theta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplot for different theta values\n",
    "plots_list = []\n",
    "\n",
    "for (i, sol) in enumerate(attractors)\n",
    "    if sol !== nothing\n",
    "        # Use second half to show settled attractor\n",
    "        start_idx = length(sol.u) ÷ 2\n",
    "        x = [u[1] for u in sol.u[start_idx:end]]  \n",
    "        y = [u[2] for u in sol.u[start_idx:end]]\n",
    "        z = [u[3] for u in sol.u[start_idx:end]]\n",
    "        \n",
    "        p = plot(x, y, z, \n",
    "                label=\"\", \n",
    "                color=:blue, alpha=0.6, linewidth=0.5,\n",
    "                title=theta_names[i],\n",
    "                xlabel=\"x\", ylabel=\"y\", zlabel=\"z\",\n",
    "                camera=(60, 30))\n",
    "        push!(plots_list, p)\n",
    "    else\n",
    "        # Empty plot for failed cases\n",
    "        p = plot(title=theta_names[i] * \" (Failed)\", \n",
    "                xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")\n",
    "        push!(plots_list, p)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Combine into 2x2 subplot\n",
    "combined_plot = plot(plots_list..., layout=(2,2), size=(800, 600))\n",
    "display(combined_plot)\n",
    "\n",
    "println(\"\\nObserve how the attractor structure changes dramatically with theta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e512d",
   "metadata": {},
   "source": [
    "## Setup Parameter Estimation Experiment\n",
    "\n",
    "Now let's set up parameter estimation to learn theta from reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a true theta value to learn (start with a challenging one)\n",
    "true_theta = 3.5  # One of the interesting cases\n",
    "println(\"True theta value to learn: $true_theta\")\n",
    "\n",
    "# Generate reference data with true theta\n",
    "u0_cal = [1.0, 1.0, 1.0]  # Standard initial conditions\n",
    "tspan_cal = (0.0, 20.0)    # Shorter time for calibration\n",
    "dt_cal = 0.01\n",
    "\n",
    "prob_ref = ODEProblem(theta_system, u0_cal, tspan_cal, [true_theta])\n",
    "sol_ref = solve(prob_ref, Tsit5(), saveat=dt_cal)\n",
    "\n",
    "# Convert to package-compatible solution format\n",
    "reference_data = hcat([u for u in sol_ref.u]...)  # 3×N matrix\n",
    "reference_solution = L63Solution(\n",
    "    reference_data,\n",
    "    sol_ref.t,\n",
    "    base_params,  # Use base parameters for the solution\n",
    "    u0_cal\n",
    ")\n",
    "\n",
    "println(\"Generated reference data: $(length(sol_ref.u)) points\")\n",
    "println(\"Reference trajectory final state: $(sol_ref.u[end])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6e02f",
   "metadata": {},
   "source": [
    "## Define Loss Function and Enzyme Gradients\n",
    "\n",
    "Set up the optimization framework with Enzyme for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59834c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function for theta parameter estimation\n",
    "function theta_loss_function(theta_params, reference_sol, base_params, u0, tspan, dt)\n",
    "    θ = theta_params[1]\n",
    "    \n",
    "    # Solve ODE with current theta guess\n",
    "    theta_sys = make_theta_system(base_params)\n",
    "    prob = ODEProblem(theta_sys, u0, tspan, [θ])\n",
    "    \n",
    "    try\n",
    "        sol = solve(prob, Tsit5(), saveat=dt)\n",
    "        \n",
    "        # Return early if solver failed\n",
    "        if sol.retcode != :Success\n",
    "            return Inf\n",
    "        end\n",
    "        \n",
    "        # Convert to comparable format\n",
    "        predicted_data = hcat([u for u in sol.u]...)\n",
    "        \n",
    "        # Use package-style loss computation\n",
    "        return sum((predicted_data .- reference_sol.trajectory).^2) / length(predicted_data)\n",
    "    catch\n",
    "        return Inf\n",
    "    end\n",
    "end\n",
    "\n",
    "# Initial parameter guess (we'll start far from true value)\n",
    "theta_guess = [1.0]  # Start with standard value\n",
    "\n",
    "println(\"Initial theta guess: $(theta_guess[1])\")\n",
    "println(\"True theta: $true_theta\")\n",
    "\n",
    "# Test initial loss\n",
    "initial_loss = theta_loss_function(theta_guess, reference_solution, base_params, u0_cal, tspan_cal, dt_cal)\n",
    "println(\"Initial loss: $initial_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08e8ef",
   "metadata": {},
   "source": [
    "## Explore the Fractal Loss Landscape\n",
    "\n",
    "Before optimization, let's explore the loss landscape to understand the fractal nature of this parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85034b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for landscape exploration\n",
    "function loss_wrapper_theta(theta_params)\n",
    "    return theta_loss_function(theta_params, reference_solution, base_params, u0_cal, tspan_cal, dt_cal)\n",
    "end\n",
    "\n",
    "# Explore loss landscape around the true value and beyond\n",
    "theta_min, theta_max = 0.5, 5.0\n",
    "n_points = 100\n",
    "theta_range = range(theta_min, theta_max, length=n_points)\n",
    "\n",
    "# Compute loss landscape (this may take a while!)\n",
    "println(\"Computing loss landscape... (this may take a few minutes)\")\n",
    "loss_landscape_theta = []\n",
    "valid_evals = 0\n",
    "\n",
    "for (i, θ) in enumerate(theta_range)\n",
    "    if i % 20 == 0\n",
    "        println(\"Progress: $(i)/$(n_points) (θ=$θ)\")\n",
    "    end\n",
    "    \n",
    "    try\n",
    "        loss_val = loss_wrapper_theta([θ])\n",
    "        push!(loss_landscape_theta, loss_val)\n",
    "        if isfinite(loss_val)\n",
    "            valid_evals += 1\n",
    "        end\n",
    "    catch e\n",
    "        push!(loss_landscape_theta, Inf)\n",
    "        println(\"  Failed at θ=$θ: $e\")\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Completed landscape computation: $valid_evals/$(n_points) valid evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89014117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fractal loss landscape\n",
    "finite_mask = isfinite.(loss_landscape_theta)\n",
    "theta_finite = theta_range[finite_mask]\n",
    "loss_finite = loss_landscape_theta[finite_mask]\n",
    "\n",
    "# Use log scale for loss to see structure better\n",
    "log_loss = log10.(loss_finite .+ 1e-16)  # Add small constant to avoid log(0)\n",
    "\n",
    "p_landscape = plot(theta_finite, log_loss,\n",
    "                  xlabel=\"θ\", ylabel=\"log₁₀(Loss)\", \n",
    "                  title=\"Fractal Loss Landscape for Theta Parameter\",\n",
    "                  linewidth=2, color=:blue, legend=false)\n",
    "\n",
    "# Mark the true value\n",
    "vline!(p_landscape, [true_theta], color=:red, linewidth=3, linestyle=:dash, \n",
    "       label=\"True θ = $true_theta\")\n",
    "\n",
    "# Mark interesting values\n",
    "for θ_mark in [1.0, 4.0, 4.6]\n",
    "    if θ_mark != true_theta\n",
    "        vline!(p_landscape, [θ_mark], color=:orange, linewidth=2, linestyle=:dot, \n",
    "               alpha=0.7, label=\"\")\n",
    "    end\n",
    "end\n",
    "\n",
    "display(p_landscape)\n",
    "println(\"\\nObserve the fractal/complex structure in the loss landscape!\")\n",
    "println(\"This demonstrates why gradient-based optimization is challenging for this parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf37e9",
   "metadata": {},
   "source": [
    "## Run Parameter Calibration with Enzyme\n",
    "\n",
    "Now attempt the challenging optimization in this fractal parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient computation using Enzyme (following package patterns)\n",
    "function compute_gradient_theta!(grad, theta_params)\n",
    "    try\n",
    "        autodiff(Reverse, loss_wrapper_theta, Active, Duplicated(theta_params, grad))\n",
    "    catch e\n",
    "        println(\"Gradient computation failed: $e\")\n",
    "        fill!(grad, 0.0)  # Zero gradient on failure\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# Package-style optimization function\n",
    "function optimize_theta(initial_theta; max_iters=100, lr=1e-5, verbose=true)\n",
    "    theta_params = copy(initial_theta)\n",
    "    grad = zeros(length(theta_params))\n",
    "    \n",
    "    best_loss = Inf\n",
    "    best_theta = copy(theta_params)\n",
    "    loss_history = Float64[]\n",
    "    \n",
    "    verbose && println(\"Starting optimization in fractal parameter space...\")\n",
    "    verbose && println(\"This may struggle due to the complex landscape...\")\n",
    "    \n",
    "    for iter in 1:max_iters\n",
    "        # Compute loss and gradient\n",
    "        current_loss = loss_wrapper_theta(theta_params)\n",
    "        compute_gradient_theta!(grad, theta_params)\n",
    "        \n",
    "        # Update parameters (gradient descent with clipping)\n",
    "        grad_norm = norm(grad)\n",
    "        if grad_norm > 1e3  # Clip large gradients\n",
    "            grad ./= grad_norm * 1e-3\n",
    "        end\n",
    "        \n",
    "        theta_params .-= lr .* grad\n",
    "        \n",
    "        # Keep theta positive and reasonable\n",
    "        theta_params[1] = max(0.1, min(10.0, theta_params[1]))\n",
    "        \n",
    "        # Track best result\n",
    "        if current_loss < best_loss && isfinite(current_loss)\n",
    "            best_loss = current_loss\n",
    "            best_theta = copy(theta_params)\n",
    "        end\n",
    "        \n",
    "        push!(loss_history, current_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose && (iter % 10 == 0 || iter == 1)\n",
    "            println(\"Iter $iter: Loss = $(round(current_loss, digits=6)), θ = $(round(theta_params[1], digits=4)), |grad| = $(round(grad_norm, digits=6))\")\n",
    "        end\n",
    "        \n",
    "        # Early stopping if gradient is too small or loss explodes\n",
    "        if grad_norm < 1e-10 || !isfinite(current_loss)\n",
    "            verbose && println(\"Stopping: gradient too small or loss non-finite\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return best_theta, best_loss, loss_history\n",
    "end\n",
    "\n",
    "# Run optimization\n",
    "try\n",
    "    optimized_theta, final_loss, loss_history_theta = optimize_theta(theta_guess)\n",
    "    \n",
    "    println(\"\\n\" * \"=\"^50)\n",
    "    println(\"OPTIMIZATION RESULTS\")\n",
    "    println(\"=\"^50)\n",
    "    println(\"Final theta: $(round(optimized_theta[1], digits=6))\")\n",
    "    println(\"True theta:  $(round(true_theta, digits=6))\")\n",
    "    println(\"Error:       $(round(optimized_theta[1] - true_theta, digits=6))\")\n",
    "    println(\"Final loss:  $(round(final_loss, digits=10))\")\n",
    "    println(\"Relative error: $(round(abs(optimized_theta[1] - true_theta) / true_theta * 100, digits=4))%\")\n",
    "    \n",
    "    global optimization_result_theta = optimized_theta\n",
    "    global optimization_loss_history = loss_history_theta\n",
    "catch e\n",
    "    println(\"\\n\" * \"=\"^50)\n",
    "    println(\"OPTIMIZATION FAILED\")\n",
    "    println(\"=\"^50)\n",
    "    println(\"Error: $e\")\n",
    "    println(\"This demonstrates the challenge of gradient computation in fractal parameter space!\")\n",
    "    global optimization_result_theta = nothing\n",
    "    global optimization_loss_history = []\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ee796",
   "metadata": {},
   "source": [
    "## Alternative Optimization Strategies\n",
    "\n",
    "Since the fractal parameter space is challenging, let's try different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple starting points to explore the landscape\n",
    "start_points = [0.8, 1.0, 1.2, 2.0, 3.0, 3.5, 4.0, 4.5]\n",
    "results_multistart = []\n",
    "\n",
    "println(\"Trying multiple starting points...\")\n",
    "for (i, θ_start) in enumerate(start_points)\n",
    "    println(\"\\nStarting point $i: θ₀ = $θ_start\")\n",
    "    \n",
    "    try\n",
    "        result_theta, result_loss, _ = optimize_theta([θ_start], max_iters=30, verbose=false)\n",
    "        \n",
    "        push!(results_multistart, (start=θ_start, final=result_theta[1], \n",
    "                                  loss=result_loss, converged=:Success))\n",
    "        \n",
    "        println(\"  Final θ: $(round(result_theta[1], digits=4))\")\n",
    "        println(\"  Loss: $(round(result_loss, digits=6))\")\n",
    "        println(\"  Status: Success\")\n",
    "        \n",
    "    catch e\n",
    "        println(\"  Failed: $e\")\n",
    "        push!(results_multistart, (start=θ_start, final=NaN, loss=Inf, converged=:Failed))\n",
    "    end\n",
    "end\n",
    "\n",
    "# Analyze results\n",
    "println(\"\\n\" * \"=\"^50)\n",
    "println(\"MULTI-START OPTIMIZATION SUMMARY\")\n",
    "println(\"=\"^50)\n",
    "println(\"True theta: $true_theta\")\n",
    "println(\"\\nResults:\")\n",
    "for (i, res) in enumerate(results_multistart)\n",
    "    error = isnan(res.final) ? \"N/A\" : round(res.final - true_theta, digits=3)\n",
    "    final_str = isnan(res.final) ? \"Failed\" : round(res.final, digits=3)\n",
    "    loss_str = isinf(res.loss) ? \"∞\" : round(res.loss, digits=2)\n",
    "    println(\"Start $(res.start) → Final $final_str | Error: $error | Loss: $loss_str\")\n",
    "end\n",
    "\n",
    "# Find best result\n",
    "valid_results = filter(r -> isfinite(r.loss), results_multistart)\n",
    "if !isempty(valid_results)\n",
    "    best_result = minimum(valid_results, by=r -> r.loss)\n",
    "    println(\"\\nBest result: θ = $(round(best_result.final, digits=4)), Loss = $(round(best_result.loss, digits=6))\")\n",
    "else\n",
    "    println(\"\\nNo successful optimizations - demonstrates the challenge of this parameter space!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f091ba",
   "metadata": {},
   "source": [
    "## Visualization and Analysis\n",
    "\n",
    "Compare results and analyze the challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "plots_analysis = []\n",
    "\n",
    "# 1. Loss landscape with optimization attempts\n",
    "p1 = plot(theta_finite, log_loss,\n",
    "          xlabel=\"θ\", ylabel=\"log₁₀(Loss)\", \n",
    "          title=\"Loss Landscape + Optimization Attempts\",\n",
    "          linewidth=2, color=:blue, label=\"Loss landscape\")\n",
    "\n",
    "vline!(p1, [true_theta], color=:red, linewidth=3, linestyle=:dash, label=\"True θ\")\n",
    "\n",
    "# Mark optimization attempts\n",
    "for res in results_multistart\n",
    "    if isfinite(res.loss)\n",
    "        scatter!(p1, [res.final], [log10(res.loss + 1e-16)], \n",
    "                marker=:circle, markersize=6, color=:green, alpha=0.7, label=\"\")\n",
    "    end\n",
    "end\n",
    "\n",
    "push!(plots_analysis, p1)\n",
    "\n",
    "# 2. Optimization convergence (if available)\n",
    "if @isdefined(optimization_loss_history) && !isempty(optimization_loss_history)\n",
    "    p2 = plot(1:length(optimization_loss_history), log10.(optimization_loss_history .+ 1e-16), \n",
    "              label=\"Loss\", color=:green, linewidth=2,\n",
    "              title=\"Optimization Convergence\", xlabel=\"Iteration\", ylabel=\"log₁₀(Loss)\")\n",
    "    push!(plots_analysis, p2)\n",
    "end\n",
    "\n",
    "# 3. Parameter recovery scatter\n",
    "successful_finals = [res.final for res in results_multistart if isfinite(res.loss)]\n",
    "successful_starts = [res.start for res in results_multistart if isfinite(res.loss)]\n",
    "\n",
    "if !isempty(successful_finals)\n",
    "    p3 = scatter(successful_starts, successful_finals,\n",
    "                xlabel=\"Starting θ\", ylabel=\"Final θ\",\n",
    "                title=\"Parameter Recovery\",\n",
    "                markersize=8, color=:purple, alpha=0.7, label=\"Optimization results\")\n",
    "    plot!(p3, [0, 5], [0, 5], linestyle=:dash, color=:black, label=\"Perfect recovery\")\n",
    "    hline!(p3, [true_theta], color=:red, linestyle=:dash, label=\"True θ\")\n",
    "    push!(plots_analysis, p3)\n",
    "end\n",
    "\n",
    "# Display analysis\n",
    "if length(plots_analysis) > 1\n",
    "    combined_analysis = plot(plots_analysis..., layout=(1, length(plots_analysis)), size=(400*length(plots_analysis), 400))\n",
    "    display(combined_analysis)\n",
    "else\n",
    "    display(plots_analysis[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742aead",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This test case demonstrates the challenges of parameter estimation in fractal parameter spaces using the `LorenzParameterEstimation` package:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Fractal Parameter Space**: The theta parameter creates a highly complex, fractal loss landscape where small changes in theta can dramatically alter the attractor structure.\n",
    "\n",
    "2. **Gradient Computation Challenges**: Computing gradients through this parameter space is extremely challenging. The loss function can become discontinuous or highly oscillatory.\n",
    "\n",
    "3. **Multiple Local Minima**: The fractal nature creates many local minima, making global optimization very difficult.\n",
    "\n",
    "4. **Attractor Collapse/Emergence**: Different theta values cause the strange attractor to collapse or reemerge, creating dramatic changes in system behavior.\n",
    "\n",
    "### Comparison with Coordinate Shifts:\n",
    "- **Coordinate shifts**: Smooth gradients, well-behaved optimization\n",
    "- **Theta parameter**: Fractal gradients, challenging optimization\n",
    "\n",
    "### Implications for the Package:\n",
    "- This test case pushes the limits of automatic differentiation\n",
    "- Highlights the robustness of the package's Enzyme.jl integration\n",
    "- Demonstrates when gradient-based methods may struggle\n",
    "- Shows the value of the package's modular design for testing edge cases\n",
    "\n",
    "### Package Design Benefits:\n",
    "- Clean separation allows testing challenging scenarios\n",
    "- Enzyme.jl integration handles gradient computation robustly\n",
    "- Modular loss functions enable custom test cases\n",
    "- Consistent patterns make it easy to extend functionality\n",
    "\n",
    "### Recommendations:\n",
    "- Consider hybrid approaches (gradient + derivative-free methods)\n",
    "- Use multiple starting points for global search\n",
    "- Implement gradient validation and fallback strategies\n",
    "- Consider regularization techniques to smooth the landscape\n",
    "\n",
    "This test case serves as an excellent stress test for parameter estimation algorithms and demonstrates both the capabilities and limitations of gradient-based optimization in challenging parameter spaces."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
